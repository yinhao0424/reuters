{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReusterSiameseNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V6pvVcsoLQJKkS5V4InbEKWRQDh9xX7J",
      "authorship_tag": "ABX9TyM2SW05WNzak+6v2dDwc4rM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinhao0424/reuters/blob/master/ReusterSiameseNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwKBtxgxHRX"
      },
      "source": [
        "\n",
        "\n",
        "## Few Shot Learner\n",
        "By building a pooling layer on top of the BERT model, the sentence embedding has been generated. The loss function is triplet loss.  \n",
        "\n",
        "The one shot learner has been tested on the \"commodity\" category. It contains two parts, comparing the support set with query without finetuning and training a finetuned classifer on support set.\n",
        "\n",
        "***\n",
        "1/2/2021\n",
        "- Build dataloader\n",
        "- Construct triplet loss\n",
        "\n",
        "1/4/2021\n",
        "- Generate support and testing set\n",
        "- Work on existing package sentence embedding\n",
        "- Build siamese NN\n",
        "\n",
        "1/5/2020\n",
        "- Debug for siamese NN\n",
        "- Review Few shot learning\n",
        "\n",
        "1/6/2021  \n",
        "Test the model performance\n",
        "- Test without Finetuning\n",
        "  - store the embedding of support set\n",
        "  - caculate the embedding of query\n",
        "  - find the sample with the highest similarity score\n",
        "- Finetuning\n",
        "***\n",
        "Reference:\n",
        "- Paper/Blog\n",
        "  - [BERT word embedding](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#31-running-bert-on-our-text)\n",
        "  - [Sentence Embeddings using Siamese BERT-Networks - paper](https://www.aclweb.org/anthology/D19-1410.pdf)\n",
        "  - [Sentence Embeddings using Siamese BERT-Networks - colab](https://github.com/aneesha/SiameseBERT-Notebook/blob/master/SiameseBERT_SemanticSearch.ipynb)\n",
        "- Disscussion\n",
        "    - [Generate sequence classifier](https://github.com/huggingface/transformers/issues/1001)\n",
        "    - [Sequence Classification pooled output vs last hidden state](https://github.com/huggingface/transformers/issues/1328)\n",
        "- Github\n",
        "  -  [triplet-network-pytorch](https://github.com/andreasveit/triplet-network-pytorch/blob/master/train.py)\n",
        "  - [One Shot Learning with Siamese NetworksÂ¶](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3viDMDjJyZ"
      },
      "source": [
        "# a specific version of transformaer has been used \n",
        "! pip install -q transformers==3.0.2\n",
        "# !pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt2fu-WpjAPH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFW4xEIzGyu9",
        "outputId": "902c805d-b8ed-4a71-bb84-365c85e54753"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLUxZch_jiHP"
      },
      "source": [
        "# config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36SizijJvWkJ"
      },
      "source": [
        "# Sections of config\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3aClwrmljPgj",
        "outputId": "1dc82f9d-07ee-44e8-d994-659bf4824cb2"
      },
      "source": [
        "reuster_train = pd.read_csv('/content/drive/MyDrive/data/reuters/reuster_fewshot_train.csv')\n",
        "reuster_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>topics</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4016</td>\n",
              "      <td>iron-steel</td>\n",
              "      <td>usx &lt;x&gt; proved oil, gas reserves fall in 1986u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4022</td>\n",
              "      <td>carcass</td>\n",
              "      <td>argentine meat exports higher in jan/feb 1987a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4022</td>\n",
              "      <td>livestock</td>\n",
              "      <td>argentine meat exports higher in jan/feb 1987a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4035</td>\n",
              "      <td>veg-oil</td>\n",
              "      <td>british minister criticises proposed ec oils t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4040</td>\n",
              "      <td>oilseed</td>\n",
              "      <td>china's rapeseed crop damaged by stormsthe yie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id      topics                                              texts\n",
              "0  4016  iron-steel  usx <x> proved oil, gas reserves fall in 1986u...\n",
              "1  4022     carcass  argentine meat exports higher in jan/feb 1987a...\n",
              "2  4022   livestock  argentine meat exports higher in jan/feb 1987a...\n",
              "3  4035     veg-oil  british minister criticises proposed ec oils t...\n",
              "4  4040     oilseed  china's rapeseed crop damaged by stormsthe yie..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkaOQbnjPuF",
        "outputId": "1aaee006-6cd4-43c6-dbcb-2353c3d22f71"
      },
      "source": [
        "reuster_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1143, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P09PmVIRuPlh"
      },
      "source": [
        "class SiameseDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Input: a dataframe\n",
        "        output: anchor, positive and negative\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.texts\n",
        "        self.topics = self.data.topics\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        anchor = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = anchor['input_ids']\n",
        "        mask = anchor['attention_mask']\n",
        "        # token_type_ids = anchor[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {'anchor':{\n",
        "            'index':torch.tensor(index, dtype=torch.int),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long)},\n",
        "        'positive': self.get_positive(index),\n",
        "        'negative': self.get_negative(index)\n",
        "        }\n",
        "\n",
        "    def get_positive(self, index):\n",
        "         # the topic\n",
        "        topic = self.topics[index]\n",
        "\n",
        "        # select positive data which have the same topic with the anchor\n",
        "        candidates = self.topics[self.topics==topic].index\n",
        "        p_idx = index\n",
        "        while p_idx == index:\n",
        "          p_idx = np.random.choice(candidates)\n",
        "        \n",
        "        text = str(self.text[p_idx])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        positive = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = positive['input_ids']\n",
        "        mask = positive['attention_mask']\n",
        "        # token_type_ids = positive[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'index':torch.tensor(p_idx, dtype=torch.int),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long)}\n",
        "\n",
        "    def get_negative(self, index):\n",
        "         # the topic\n",
        "        topic = self.topics[index]\n",
        "\n",
        "        # select positive data which have the same topic with the anchor\n",
        "        candidates = self.topics[self.topics!=topic].index\n",
        "        n_idx = index\n",
        "        n_idx = np.random.choice(candidates)\n",
        "        \n",
        "        text = str(self.text[n_idx])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        negative = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = negative['input_ids']\n",
        "        mask = negative['attention_mask']\n",
        "        # token_type_ids = negative[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'index':torch.tensor(n_idx, dtype=torch.int),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQNXOPslqo6",
        "outputId": "7cce873c-f7f3-473d-fdeb-5342eae63939"
      },
      "source": [
        "print(\"TRAIN Dataset: {}\".format(reuster_train.shape))\n",
        "\n",
        "training_set = SiameseDataset(reuster_train, tokenizer, MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (1143, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKdMcmgovsnK"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 1\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 1\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdrZU8QuvvSl",
        "outputId": "fb925e80-c878-452f-8588-fcd8e668c514"
      },
      "source": [
        "print(\"The len of training loader is {}.\".format(len(training_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The len of training loader is 143.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8c5Z5-qwJ8U"
      },
      "source": [
        "## Create the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9OVcyyv4KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bb9b12-027f-4eeb-8c11-4a6b52a23877"
      },
      "source": [
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, data):\n",
        "        input_ids = data['ids']\n",
        "        attention_mask = data['mask']\n",
        "\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "    # def forward(self, anchor,positive,negative):\n",
        "    #     res_anchor = self.forward_once(anchor)\n",
        "    #     res_positive = self.forward_once(positive)\n",
        "    #     res_negative = self.forward_once(negative)\n",
        "    #     return res_anchor,res_positive,res_negative\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=256, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAjnMgenGsQg"
      },
      "source": [
        "def triplet_loss(anchor, positive, negative):\n",
        "  loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "  return loss(anchor, positive, negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3xlIIfeGsW4"
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jL2Zi_0GsbL"
      },
      "source": [
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        \n",
        "        anchor = {key:data['anchor'][key].cuda() for key in data['anchor']}\n",
        "        positive = {key:data['positive'][key].cuda() for key in data['positive']}\n",
        "        negative = {key:data['negative'][key].cuda() for key in data['negative']}\n",
        "        res_anchor,res_positive,res_negative = model(anchor),model(positive),model(negative)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = triplet_loss(res_anchor,res_positive,res_negative)\n",
        "        if _%20==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB72xqrfE9ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e6ba8c-1ccd-411d-d667-8381e39ed539"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.949222207069397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "20it [00:12,  1.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.6064987182617188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40it [00:24,  1.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.7553625106811523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60it [00:36,  1.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.3175418972969055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "80it [00:48,  1.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.2462424635887146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100it [01:01,  1.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.5698609352111816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "120it [01:13,  1.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.3806683123111725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140it [01:25,  1.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.24013476073741913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "143it [01:27,  1.64it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.03964850306510925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "20it [00:11,  1.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.0872943103313446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40it [00:23,  1.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.13975036144256592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "60it [00:36,  1.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.8357698917388916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "80it [00:48,  1.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.3819858729839325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100it [01:00,  1.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.1067567765712738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "120it [01:12,  1.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.14656051993370056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "140it [01:24,  1.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "143it [01:25,  1.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckg9SvZEfKF6",
        "outputId": "47f68b05-ad61-4b7c-e607-6793fdcd3c10"
      },
      "source": [
        "# save model \n",
        "PATH = '/content/drive/MyDrive/data/reuters/siamese_NN.pth'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "print('Saved')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAQ9kpzdE3AU"
      },
      "source": [
        "## Test similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4pp5dZ8Ezpo"
      },
      "source": [
        "reuster_support = pd.read_csv('/content/drive/MyDrive/data/reuters/fewshot_support.csv')\n",
        "reuster_test = pd.read_csv('/content/drive/MyDrive/data/reuters/fewshot_test.csv')\n",
        "reuster_support.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3P1y91uE0PQ"
      },
      "source": [
        "class OneShotLearning(Dataset):\n",
        "    \"\"\"\n",
        "        Input: a dataframe\n",
        "        output: index, ids, mask\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.texts\n",
        "        self.topics = self.data.topics\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        anchor = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = anchor['input_ids']\n",
        "        mask = anchor['attention_mask']\n",
        "        # token_type_ids = anchor[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'index':torch.tensor(index, dtype=torch.int),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgWhfk7N0rJw"
      },
      "source": [
        "print(\"Support Dataset: {}\".format(reuster_support.shape))\n",
        "print(\"Test Dataset: {}\".format(reuster_test.shape))\n",
        "\n",
        "\n",
        "support_set = OneShotLearning(reuster_support, tokenizer, MAX_LEN)\n",
        "testing_set = OneShotLearning(reuster_test, tokenizer, MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kYNpxpZ0v7U"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 8\n",
        "SUPPORT_BATCH_SIZE = 1\n",
        "TEST_BATCH_SIZE = 1\n",
        "\n",
        "support_params = {'batch_size': SUPPORT_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "support_loader = DataLoader(support_set, **support_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4T1baq0v_5"
      },
      "source": [
        "# generate support embedding\n",
        "support_res = []\n",
        "support_idx = []\n",
        "for _,data in tqdm(enumerate(support_loader, 0)):\n",
        "    support_idx.append(data['index'].tolist()[0])\n",
        "    support = {key:data[key].cuda() for key in data}\n",
        "\n",
        "    support_res.append(model(support))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rkd7rqU0wCa"
      },
      "source": [
        "def similar_support(testing_res):\n",
        "  most_similar = 0\n",
        "  most_similar_idx = None\n",
        "  for idx,support in enumerate(support_res):\n",
        "    out = cosine_similarity(support,testing_res)\n",
        "    if out > most_similar:\n",
        "      most_similar = out\n",
        "      most_similar_idx = idx\n",
        "  return most_similar_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkt3xNB0wG6"
      },
      "source": [
        "true_positive = 0\n",
        "for index, test in enumerate(testing_res):\n",
        "  if index == 10:\n",
        "    break\n",
        "  most_similar_idx = similar_support(test)\n",
        "  support_topic = reuster_support.iloc[support_idx[most_similar_idx]]['topics']\n",
        "  test_topic = reuster_test.iloc[testing_idx[index]]['topics']\n",
        "  if support_topic == test_topic:\n",
        "    true_positive+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1haZQnxF3HB8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}